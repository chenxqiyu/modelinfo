# F:\ai\ComfyUI-aki-v1.3\ComfyUI-aki-v1.3\python
# pip install tkinterdnd2   
import os
import sys
import gguf
import torch
from safetensors import safe_open
from safetensors.torch import load_file
from collections import Counter

import numpy as np
import tkinter as tk
from tkinterdnd2 import TkinterDnD, DND_FILES
from llama_cpp import Llama

hang=25
class StdoutRedirector:
    def __init__(self, text_widget):
        self.text_widget = text_widget

    def write(self, string):
        self.text_widget.insert(tk.END, string)
        self.text_widget.see(tk.END)

    def flush(self):
        pass

# GGUF æƒé‡ç±»å‹æ˜ å°„è¡¨
GGUF_WEIGHT_TYPES = {
    0:  "float32",
    1:  "float16",
    2:  "Q4_0",
    3:  "Q4_1",
    6:  "Q5_0",
    7:  "Q5_1",
    8:  "Q8_0", 
    9:  "Q8_1",
    10: "Q2_K",
    11: "Q3_K",
    12: "Q4_K",
    13: "Q5_K",
    14: "Q6_K",
}

def format_param_count_practical(param_count):
    """å®ç”¨ç‰ˆæœ¬ï¼Œæ ¹æ®è§„æ¨¡æ™ºèƒ½é€‰æ‹©æ˜¾ç¤ºæ–¹å¼"""
    if param_count == 0:
        return "0"
    
    billions = param_count / 1_000_000_000
    
    if billions >= 1:  # 1Bä»¥ä¸Š
        if billions >= 1000:  # 1000Bä»¥ä¸Šç”¨T
            trillions = billions / 1000
            return f"{trillions:.1f}T" if trillions < 10 else f"{int(trillions)}T"
        elif billions >= 100:  # 100Bä»¥ä¸Šæ˜¾ç¤ºæ•´æ•°
            return f"{int(billions)}B"
        elif billions >= 10:  # 10-100Bæ˜¾ç¤ºä¸€ä½å°æ•°
            return f"{billions:.1f}B"
        else:  # 1-10Bæ˜¾ç¤ºä¸¤ä½å°æ•°
            return f"{billions:.2f}B"
    else:  # 1Bä»¥ä¸‹
        millions = param_count / 1_000_000
        if param_count >= 1_000_000:  # 1Mä»¥ä¸Šç”¨M
            return f"{millions:.1f}M" if millions < 100 else f"{int(millions)}M"
        elif param_count >= 1_000:  # 1Kä»¥ä¸Šç”¨K
            thousands = param_count / 1_000
            return f"{int(thousands)}K"
        else:
            return f"{param_count}"

def calculate_memory_requirement(P, Q):
    """
    æ ¹æ®å›¾ç‰‡å…¬å¼è®¡ç®—æ˜¾å­˜éœ€æ±‚: M = (P Ã— Q) / 8 Ã— 1.2
    
    Args:
        P: æ¨¡å‹å‚æ•°é‡ (å•ä½: äº¿)
        Q: å‚æ•°ä½å®½ (FP16=16, INT8=8, INT4=4)
    
    Returns:
        æ˜¾å­˜éœ€æ±‚ (å•ä½: GB)
    """
    # P éœ€è¦è½¬æ¢ä¸ºä»¥äº¿ä¸ºå•ä½çš„æ•°å€¼
    P_billions = P / 1_000_000_000  # è½¬æ¢ä¸ºBå•ä½
    M = (P_billions * Q) / 8 * 1.2
    return M

def get_quantization_bits(dtype):
    """æ ¹æ®æ•°æ®ç±»å‹è·å–å‚æ•°ä½å®½Qå€¼"""
    if dtype == torch.float32:
        return 32
    elif dtype == torch.float16:
        return 16
    elif dtype == torch.bfloat16:
        return 16  # bfloat16ä¹Ÿæ˜¯16ä½
    elif dtype == torch.float8_e5m2:
        return 8
    elif dtype == torch.float8_e4m3fn:
        return 8
    elif 'int8' in str(dtype).lower() or 'q8' in str(dtype).lower():
        return 8
    elif 'int6' in str(dtype).lower() or 'q6' in str(dtype).lower():
        return 6
    elif 'int5' in str(dtype).lower() or 'q5' in str(dtype).lower():
        return 5
    elif 'int4' in str(dtype).lower() or 'q4' in str(dtype).lower():
        return 4
    elif 'int3' in str(dtype).lower() or 'q3' in str(dtype).lower():
        return 3
    elif 'int2' in str(dtype).lower() or 'q2' in str(dtype).lower():
        return 2
    else:
        return 16  # é»˜è®¤æŒ‰FP16å¤„ç†

def classify_model_size(total_params):
    """æ ¹æ®æ€»å‚æ•°é‡å¯¹æ¨¡å‹è¿›è¡Œåˆ†ç±»"""
    if total_params >= 100_000_000_000:  # 100B+
        return "è¶…å¤§è§„æ¨¡æ¨¡å‹ (>100Bå‚æ•°)"
    elif total_params >= 50_000_000_000:  # 50B+
        return "è¶…å¤§æ¨¡å‹ (50B-100Bå‚æ•°)"
    elif total_params >= 10_000_000_000:  # 10B+
        return "å¤§æ¨¡å‹ (10B-50Bå‚æ•°)"
    elif total_params >= 1_000_000_000:   # 1B+
        return "ä¸­ç­‰æ¨¡å‹ (1B-10Bå‚æ•°)"
    elif total_params >= 100_000_000:     # 100M+
        return "å°å‹æ¨¡å‹ (100M-1Bå‚æ•°)"
    else:
        return "å¾®å‹æ¨¡å‹ (<100Må‚æ•°)"

def inspect_safetensors(filepath):
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        # åŠ è½½safetensorsæ–‡ä»¶ï¼ˆå¸¦å…ƒæ•°æ®ï¼‰
        with open(filepath, "rb") as f:
            from safetensors import safe_open
            tensors = load_file(filepath)
            # æ‰“å¼€æ–‡ä»¶ä»¥è¯»å–å…ƒæ•°æ®
            metadata = {}
            try:
                with safe_open(filepath, framework="pt", device="cpu") as f:
                    metadata = f.metadata()
            except:
                # å¦‚æœæ— æ³•è¯»å–å…ƒæ•°æ®ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                pass
        
        print(f"âœ… æˆåŠŸåŠ è½½æ–‡ä»¶ï¼ŒåŒ…å« {len(tensors)} ä¸ªå¼ é‡")
        
        # ç»Ÿè®¡dtypeå’Œå‚æ•°é‡
        dtype_param_count = {}
        total_params = 0
        
        # ç»Ÿè®¡tensoråç§°
        tensor_names = []
        tensor_prefixes = []  # å­˜å‚¨æ¯ä¸ªtensoråç§°çš„ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼ˆç¬¬ä¸€ä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
        tensor_second_prefixes = []  # å­˜å‚¨æ¯ä¸ªtensoråç§°çš„ç¬¬äºŒéƒ¨åˆ†ï¼ˆç¬¬ä¸€ä¸ªç‚¹å’Œç¬¬äºŒä¸ªç‚¹ä¹‹é—´çš„éƒ¨åˆ†ï¼‰
        tensor_third_prefixes = []  # å­˜å‚¨æ¯ä¸ªtensoråç§°çš„ç¬¬ä¸‰éƒ¨åˆ†ï¼ˆç¬¬äºŒä¸ªç‚¹å’Œç¬¬ä¸‰ä¸ªç‚¹ä¹‹é—´çš„éƒ¨åˆ†ï¼‰
        
        for name, tensor in tensors.items():
            dtype = tensor.dtype
            param_count = tensor.numel()
            dtype_param_count[dtype] = dtype_param_count.get(dtype, 0) + param_count
            total_params += param_count
            tensor_names.append(name)
            
            # æå–åç§°ä¸­çš„ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼ˆç¬¬ä¸€ä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            prefix = name.split('.')[0] if '.' in name else name
            tensor_prefixes.append(prefix)
            
            # æå–åç§°ä¸­çš„ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼ˆç¬¬ä¸€ä¸ªç‚¹ä¹‹åã€ç¬¬äºŒä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            parts = name.split('.')
            if len(parts) >= 2:
                second_prefix = parts[1]  # ç¬¬äºŒä¸ªéƒ¨åˆ†
            else:
                second_prefix = ""  # å¦‚æœæ²¡æœ‰ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼Œåˆ™ä¸ºç©ºå­—ç¬¦ä¸²
            tensor_second_prefixes.append(second_prefix)
            
            # æå–åç§°ä¸­çš„ç¬¬ä¸‰ä¸ªéƒ¨åˆ†ï¼ˆç¬¬äºŒä¸ªç‚¹ä¹‹åã€ç¬¬ä¸‰ä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            if len(parts) >= 3:
                third_prefix = parts[2]  # ç¬¬ä¸‰ä¸ªéƒ¨åˆ†
            else:
                third_prefix = ""  # å¦‚æœæ²¡æœ‰ç¬¬ä¸‰ä¸ªéƒ¨åˆ†ï¼Œåˆ™ä¸ºç©ºå­—ç¬¦ä¸²
            tensor_third_prefixes.append(third_prefix)
        
        # è®¡ç®—å”¯ä¸€tensoråç§°çš„æ•°é‡
        unique_names = list(dict.fromkeys(tensor_names))  # ä¿æŒé¦–æ¬¡å‡ºç°çš„é¡ºåº
        duplicate_names = [name for name in tensor_names if tensor_names.count(name) > 1]
        unique_name_count = len(unique_names)
        
        # è®¡ç®—å”¯ä¸€å‰ç¼€çš„æ•°é‡
        unique_prefixes = list(dict.fromkeys(tensor_prefixes))  # ä¿æŒé¦–æ¬¡å‡ºç°çš„é¡ºåº
        prefix_counts = Counter(tensor_prefixes)  # è®¡ç®—æ¯ä¸ªå‰ç¼€å‡ºç°çš„æ¬¡æ•°
        unique_prefix_count = len(unique_prefixes)
        
        # è®¡ç®—å”¯ä¸€ç¬¬äºŒå‰ç¼€çš„æ•°é‡
        unique_second_prefixes = [p for p in dict.fromkeys(tensor_second_prefixes) if p]  # ä¿æŒé¦–æ¬¡å‡ºç°çš„é¡ºåºï¼Œæ’é™¤ç©ºå­—ç¬¦ä¸²
        second_prefix_counts = Counter([p for p in tensor_second_prefixes if p])  # è®¡ç®—æ¯ä¸ªç¬¬äºŒå‰ç¼€å‡ºç°çš„æ¬¡æ•°
        unique_second_prefix_count = len(unique_second_prefixes)
        
        # è®¡ç®—å”¯ä¸€ç¬¬ä¸‰å‰ç¼€çš„æ•°é‡
        unique_third_prefixes = [p for p in dict.fromkeys(tensor_third_prefixes) if p]  # ä¿æŒé¦–æ¬¡å‡ºç°çš„é¡ºåºï¼Œæ’é™¤ç©ºå­—ç¬¦ä¸²
        third_prefix_counts = Counter([p for p in tensor_third_prefixes if p])  # è®¡ç®—æ¯ä¸ªç¬¬ä¸‰å‰ç¼€å‡ºç°çš„æ¬¡æ•°
        unique_third_prefix_count = len(unique_third_prefixes)
        
        # è®¡ç®—ä¸»è¦ç²¾åº¦çš„ä½å®½
        if dtype_param_count:
            main_dtype = max(dtype_param_count, key=dtype_param_count.get)
            main_percentage = (dtype_param_count[main_dtype] / total_params) * 100
            Q_value = get_quantization_bits(main_dtype)
            
            # ä½¿ç”¨ä¸»è¦ç²¾åº¦è®¡ç®—æ˜¾å­˜éœ€æ±‚
            memory_gb = calculate_memory_requirement(total_params, Q_value)
        else:
            memory_gb = 0
            Q_value = 16  # é»˜è®¤å€¼
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"ğŸ“„ æ–‡ä»¶: {os.path.basename(filepath)}\n"


        report += f"\n{'â”€' * hang}\n"  # åˆ†éš”çº¿
        # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
        if metadata:
            report += f"ğŸ“š å…ƒæ•°æ®ä¿¡æ¯:\n"
            for key, value in list(metadata.items())[:10]:  # æ˜¾ç¤ºå‰10ä¸ªå…ƒæ•°æ®é¡¹
                report += f"   {key}: {value}\n"
            if len(metadata) > 10:
                report += f"   ... è¿˜æœ‰ {len(metadata) - 10} ä¸ªå…ƒæ•°æ®é¡¹\n\n"
            else:
                report += "\n"
        else:
            report += f"ğŸ“š å…ƒæ•°æ®: æ— \n\n"

        report += f"\n{'â”€' * hang}\n"  # åˆ†éš”çº¿

        # æ˜¾ç¤ºå‰å‡ ä¸ªå”¯ä¸€çš„tensoråç§°
        if unique_names:
            report += f"ğŸ·ï¸ å‰10ä¸ªå”¯ä¸€å¼ é‡åç§°:\n"
            for i, name in enumerate(unique_names[:10]):
                report += f"   {i+1}. {name}\n"
            if len(unique_names) > 10:
                report += f"   ... è¿˜æœ‰ {len(unique_names) - 10} ä¸ªåç§°\n\n"
            else:
                report += "\n"
        
        report += f"{'â”€' * hang}\n"  # åˆ†éš”çº¿
        # æ˜¾ç¤ºå‰å‡ ä¸ªå”¯ä¸€çš„å‰ç¼€ï¼ˆç¬¬ä¸€æ¬¡å‡ºç°çš„å‰ç¼€ï¼‰
        if unique_prefixes:
            report += f"ğŸ·ï¸ ç¬¬ä¸€å‰ç¼€ç»Ÿè®¡ (æŒ‰é¦–æ¬¡å‡ºç°é¡ºåº):\n"
            for i, prefix in enumerate(unique_prefixes[:10]):  # æ˜¾ç¤ºå‰10ä¸ªä¸åŒçš„å‰ç¼€
                count = prefix_counts[prefix]
                report += f"   {i+1}. {prefix} ({count} ä¸ªå¼ é‡)\n"
            if len(unique_prefixes) > 10:
                report += f"   ... è¿˜æœ‰ {len(unique_prefixes) - 10} ä¸ªå‰ç¼€\n\n"
            else:
                report += "\n"
        
        report += f"{'â”€' * hang}\n"  # åˆ†éš”çº¿
        # æ˜¾ç¤ºå‰å‡ ä¸ªå”¯ä¸€çš„ç¬¬äºŒå‰ç¼€ï¼ˆç¬¬ä¸€æ¬¡å‡ºç°çš„ç¬¬äºŒå‰ç¼€ï¼‰
        if unique_second_prefixes:
            report += f"ğŸ·ï¸ ç¬¬äºŒå‰ç¼€ç»Ÿè®¡ (æŒ‰é¦–æ¬¡å‡ºç°é¡ºåº):\n"
            for i, prefix in enumerate(unique_second_prefixes[:10]):  # æ˜¾ç¤ºå‰10ä¸ªä¸åŒçš„ç¬¬äºŒå‰ç¼€
                count = second_prefix_counts[prefix]
                report += f"   {i+1}. {prefix} ({count} ä¸ªå¼ é‡)\n"
            if len(unique_second_prefixes) > 10:
                report += f"   ... è¿˜æœ‰ {len(unique_second_prefixes) - 10} ä¸ªç¬¬äºŒå‰ç¼€\n\n"
            else:
                report += "\n"
        
        report += f"{'â”€' * hang}\n"  # åˆ†éš”çº¿
        # æ˜¾ç¤ºå‰å‡ ä¸ªå”¯ä¸€çš„ç¬¬ä¸‰å‰ç¼€ï¼ˆç¬¬ä¸€æ¬¡å‡ºç°çš„ç¬¬ä¸‰å‰ç¼€ï¼‰
        if unique_third_prefixes:
            report += f"ğŸ·ï¸ ç¬¬ä¸‰å‰ç¼€ç»Ÿè®¡ (æŒ‰é¦–æ¬¡å‡ºç°é¡ºåº):\n"
            for i, prefix in enumerate(unique_third_prefixes[:10]):  # æ˜¾ç¤ºå‰10ä¸ªä¸åŒçš„ç¬¬ä¸‰å‰ç¼€
                count = third_prefix_counts[prefix]
                report += f"   {i+1}. {prefix} ({count} ä¸ªå¼ é‡)\n"
            if len(unique_third_prefixes) > 10:
                report += f"   ... è¿˜æœ‰ {len(unique_third_prefixes) - 10} ä¸ªç¬¬ä¸‰å‰ç¼€\n\n"
            else:
                report += "\n"



        
        report += f"{'â”€' * hang}\n"  # åˆ†éš”çº¿
        report += f"ğŸ“Š æ€»å‚æ•°é‡: {total_params:,} ({format_param_count_practical(total_params)})\n"
        report += f"ğŸ“ˆ å¼ é‡æ•°é‡: {len(tensors)} (å”¯ä¸€åç§°: {unique_name_count}, é‡å¤åç§°: {len(duplicate_names)})\n"
        report += f"ğŸ·ï¸ å‰ç¼€ç»Ÿè®¡: {unique_prefix_count} ä¸ªä¸åŒç¬¬ä¸€å‰ç¼€, {unique_second_prefix_count} ä¸ªä¸åŒç¬¬äºŒå‰ç¼€, {unique_third_prefix_count} ä¸ªä¸åŒç¬¬ä¸‰å‰ç¼€\n"
        report += f"ğŸ’¾ æ˜¾å­˜ä¼°ç®—: {memory_gb:.1f} GB (åŸºäºå…¬å¼: M = (P Ã— Q) / 8 Ã— 1.2)\n"
        report += f"   - P = {total_params / 1_000_000_000:.1f}B (å‚æ•°é‡)\n"
        report += f"   - Q = {Q_value} (ä¸»è¦ç²¾åº¦: {main_dtype})\n\n"
        

        # æŒ‰å‚æ•°é‡æ’åºæ˜¾ç¤º
        sorted_dtypes = sorted(dtype_param_count.items(), key=lambda x: x[1], reverse=True)
        
        for dtype, param_count in sorted_dtypes:
            percentage = (param_count / total_params) * 100
            formatted_count = format_param_count_practical(param_count)
            q_bits = get_quantization_bits(dtype)
            report += f"ğŸ”¹ {dtype}: {param_count:,} å‚æ•° ({formatted_count}, {percentage:.2f}%, Q={q_bits})\n"
        
        # åˆ¤æ–­ç²¾åº¦ç±»å‹
        dtypes = list(dtype_param_count.keys())
        if all(dtype == torch.float32 for dtype in dtypes):
            report += "\nâœ… æ¨¡å‹ä¸ºçº¯ FP32ï¼ˆfloat32ï¼‰"
        elif all(dtype == torch.float16 for dtype in dtypes):
            report += "\nâœ… æ¨¡å‹ä¸ºçº¯ FP16ï¼ˆfloat16ï¼‰"
        elif all(dtype == torch.bfloat16 for dtype in dtypes):
            report += "\nâœ… æ¨¡å‹ä¸ºçº¯ BF16ï¼ˆbfloat16ï¼‰"
        else:
            report += f"\nâš ï¸ æ¨¡å‹ä¸ºæ··åˆç²¾åº¦ï¼ˆä¸»è¦ç²¾åº¦: {main_dtype}, å æ¯”: {main_percentage:.1f}%ï¼‰"

        # æ˜¾ç¤ºä¸åŒç²¾åº¦çš„æ˜¾å­˜éœ€æ±‚å¯¹æ¯”
        report += f"\n\nğŸ” ä¸åŒç²¾åº¦æ˜¾å­˜éœ€æ±‚å¯¹æ¯”:"
        for bits, precision_name in [(32, "FP32"), (16, "FP16/BF16"), (8, "INT8"), (4, "INT4")]:
            mem_req = calculate_memory_requirement(total_params, bits)
            report += f"\n   {precision_name}: {mem_req:.1f} GB"



        report += f"\n{'â”€' * hang}\n"  # åˆ†éš”çº¿


        # ä¿å­˜åˆ†æç»“æœåˆ° .checkinfo æ–‡ä»¶
        checkinfo_filename = filepath.rsplit('.', 1)[0] + '.checkinfo'
        try:
            with open(checkinfo_filename, 'w', encoding='utf-8') as f:
                f.write(report)
            print(report)
            print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {checkinfo_filename}")
        except Exception as e:
            print(report)
            print(f"âš ï¸  ä¿å­˜ .checkinfo æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        
        return dtype_param_count, total_params, memory_gb
        
    except Exception as e:
        error_msg = f"âŒ æ— æ³•è¯»å–æ¨¡å‹æ–‡ä»¶ {filepath}:\n{str(e)}"
        print(error_msg)
        return {}, 0, 0

# ä¸“é—¨ç”¨äºGGUFæ–‡ä»¶çš„æ˜¾å­˜ä¼°ç®—ï¼ˆå¤„ç†é‡åŒ–ç±»å‹ï¼‰
def inspect_gguf(path):
    """æ£€æŸ¥GGUFæ–‡ä»¶å¹¶è®¡ç®—æ˜¾å­˜éœ€æ±‚"""
    try:
        import gguf
        import numpy as np
        
        if not os.path.exists(path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        reader = gguf.GGUFReader(path)
        print(f"âœ… æˆåŠŸåŠ è½½GGUFæ–‡ä»¶: {os.path.basename(path)}")
        
        # è¯»å–GGUFæ–‡ä»¶çš„å…ƒæ•°æ®
        metadata = {}
        for field_name, field_value in reader.fields.items():
            try:
                # è·å–å­—æ®µå€¼
                if hasattr(field_value, 'tolist'):
                    # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨æˆ–æ ‡é‡
                    value = field_value.tolist()
                elif hasattr(field_value, 'value'):
                    # å¦‚æœæ˜¯GGUFç‰¹å®šç±»å‹ï¼Œè·å–å…¶å€¼
                    value = field_value.value
                else:
                    # ç›´æ¥ä½¿ç”¨å€¼
                    value = field_value
                metadata[field_name] = value
            except:
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å€¼çš„å­—ç¬¦ä¸²è¡¨ç¤º
                metadata[field_name] = str(field_value)
        
        # ç»Ÿè®¡dtypeå’Œå‚æ•°é‡
        dtype_param_count = {}
        total_params = 0
        
        # ç»Ÿè®¡tensoråç§°
        tensor_names = []
        tensor_prefixes = []  # å­˜å‚¨æ¯ä¸ªtensoråç§°çš„ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼ˆç¬¬ä¸€ä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
        tensor_second_prefixes = []  # å­˜å‚¨æ¯ä¸ªtensoråç§°çš„ç¬¬äºŒéƒ¨åˆ†ï¼ˆç¬¬ä¸€ä¸ªç‚¹å’Œç¬¬äºŒä¸ªç‚¹ä¹‹é—´çš„éƒ¨åˆ†ï¼‰
        tensor_third_prefixes = []  # å­˜å‚¨æ¯ä¸ªtensoråç§°çš„ç¬¬ä¸‰éƒ¨åˆ†ï¼ˆç¬¬äºŒä¸ªç‚¹å’Œç¬¬ä¸‰ä¸ªç‚¹ä¹‹é—´çš„éƒ¨åˆ†ï¼‰
        
        for tensor in reader.tensors:
            param_count = np.prod(tensor.shape)
            dtype_name = tensor.tensor_type.name
            dtype_param_count[dtype_name] = dtype_param_count.get(dtype_name, 0) + param_count
            total_params += param_count
            tensor_names.append(tensor.name)  # è®°å½•tensoråç§°
            
            # æå–åç§°ä¸­çš„ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼ˆç¬¬ä¸€ä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            prefix = tensor.name.split('.')[0] if '.' in tensor.name else tensor.name
            tensor_prefixes.append(prefix)
            
            # æå–åç§°ä¸­çš„ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼ˆç¬¬ä¸€ä¸ªç‚¹ä¹‹åã€ç¬¬äºŒä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            parts = tensor.name.split('.')
            if len(parts) >= 2:
                second_prefix = parts[1]  # ç¬¬äºŒä¸ªéƒ¨åˆ†
            else:
                second_prefix = ""  # å¦‚æœæ²¡æœ‰ç¬¬äºŒä¸ªéƒ¨åˆ†ï¼Œåˆ™ä¸ºç©ºå­—ç¬¦ä¸²
            tensor_second_prefixes.append(second_prefix)
            
            # æå–åç§°ä¸­çš„ç¬¬ä¸‰ä¸ªéƒ¨åˆ†ï¼ˆç¬¬äºŒä¸ªç‚¹ä¹‹åã€ç¬¬ä¸‰ä¸ªç‚¹ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            if len(parts) >= 3:
                third_prefix = parts[2]  # ç¬¬ä¸‰ä¸ªéƒ¨åˆ†
            else:
                third_prefix = ""  # å¦‚æœæ²¡æœ‰ç¬¬ä¸‰ä¸ªéƒ¨åˆ†ï¼Œåˆ™ä¸ºç©ºå­—ç¬¦ä¸²
            tensor_third_prefixes.append(third_prefix)
        
        # è®¡ç®—å”¯ä¸€tensoråç§°çš„æ•°é‡
        unique_names = list(dict.fromkeys(tensor_names))  # ä¿æŒé¦–æ¬¡å‡ºç°çš„é¡ºåº
        duplicate_names = [name for name in tensor_names if tensor_names.count(name) > 1]
        unique_name_count = len(unique_names)
        
        # è®¡ç®—å”¯ä¸€å‰ç¼€çš„æ•°é‡
        unique_prefixes = list(dict.fromkeys(tensor_prefixes))  # ä¿æŒé¦–æ¬¡å‡ºç°çš„é¡ºåº
        prefix_counts = Counter(tensor_prefixes)  # è®¡ç®—æ¯ä¸ªå‰ç¼€å‡ºç°çš„æ¬¡æ•°
        unique_prefix_count = len(unique_prefixes)
        
        # è®¡ç®—å”¯ä¸€ç¬¬äºŒå‰ç¼€çš„æ•°é‡
        unique_second_prefixes = [p for p in dict.fromkeys(tensor_second_prefixes) if p]  # ä¿æŒé¦–æ¬¡å‡ºç°çš„é¡ºåºï¼Œæ’é™¤ç©ºå­—ç¬¦ä¸²
        second_prefix_counts = Counter([p for p in tensor_second_prefixes if p])  # è®¡ç®—æ¯ä¸ªç¬¬äºŒå‰ç¼€å‡ºç°çš„æ¬¡æ•°
        unique_second_prefix_count = len(unique_second_prefixes)
        
        # è®¡ç®—å”¯ä¸€ç¬¬ä¸‰å‰ç¼€çš„æ•°é‡
        unique_third_prefixes = [p for p in dict.fromkeys(tensor_third_prefixes) if p]  # ä¿æŒé¦–æ¬¡å‡ºç°çš„é¡ºåºï¼Œæ’é™¤ç©ºå­—ç¬¦ä¸²
        third_prefix_counts = Counter([p for p in tensor_third_prefixes if p])  # è®¡ç®—æ¯ä¸ªç¬¬ä¸‰å‰ç¼€å‡ºç°çš„æ¬¡æ•°
        unique_third_prefix_count = len(unique_third_prefixes)
        
        # è®¡ç®—ä¸»è¦é‡åŒ–ç±»å‹çš„ä½å®½
        if dtype_param_count:
            main_dtype = max(dtype_param_count, key=dtype_param_count.get)
            main_percentage = (dtype_param_count[main_dtype] / total_params) * 100
            
            # GGUFé‡åŒ–ç±»å‹æ˜ å°„åˆ°ä½å®½
            if 'Q4' in main_dtype or 'IQ4' in main_dtype:
                Q_value = 4
            elif 'Q8' in main_dtype or 'IQ8' in main_dtype:
                Q_value = 8
            elif 'Q2' in main_dtype:
                Q_value = 2
            elif main_dtype in ['F16', 'BF16']:
                Q_value = 16
            elif main_dtype == 'F32':
                Q_value = 32
            else:
                Q_value = 16  # é»˜è®¤å€¼
            
            memory_gb = calculate_memory_requirement(total_params, Q_value)
        else:
            memory_gb = 0
            Q_value = 16
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"ğŸ“„ GGUFæ–‡ä»¶: {os.path.basename(path)}\n"
        report += f"\n{'â”€' * hang}\n"  # åˆ†éš”çº¿
        # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
        if metadata:
            report += f"ğŸ“š å…ƒæ•°æ®ä¿¡æ¯:\n"
            for key, value in list(metadata.items())[:10]:  # æ˜¾ç¤ºå‰10ä¸ªå…ƒæ•°æ®é¡¹
                report += f"   {key}: {value}\n"
            if len(metadata) > 10:
                report += f"   ... è¿˜æœ‰ {len(metadata) - 10} ä¸ªå…ƒæ•°æ®é¡¹\n\n"
            else:
                report += "\n"
        else:
            report += f"ğŸ“š å…ƒæ•°æ®: æ— \n\n"
        
        report += f"\n{'â”€' * hang}\n"  # åˆ†éš”çº¿
        # æ˜¾ç¤ºå‰å‡ ä¸ªå”¯ä¸€çš„tensoråç§°
        if unique_names:
            report += f"ğŸ·ï¸ å‰10ä¸ªå”¯ä¸€å¼ é‡åç§°:\n"
            for i, name in enumerate(unique_names[:10]):
                report += f"   {i+1}. {name}\n"
            if len(unique_names) > 10:
                report += f"   ... è¿˜æœ‰ {len(unique_names) - 10} ä¸ªåç§°\n\n"
            else:
                report += "\n"
        
        report += f"{'â”€' * hang}\n"  # åˆ†éš”çº¿
        # æ˜¾ç¤ºå‰å‡ ä¸ªå”¯ä¸€çš„å‰ç¼€ï¼ˆç¬¬ä¸€æ¬¡å‡ºç°çš„å‰ç¼€ï¼‰
        if unique_prefixes:
            report += f"ğŸ·ï¸ ç¬¬ä¸€å‰ç¼€ç»Ÿè®¡ (æŒ‰é¦–æ¬¡å‡ºç°é¡ºåº):\n"
            for i, prefix in enumerate(unique_prefixes[:10]):  # æ˜¾ç¤ºå‰10ä¸ªä¸åŒçš„å‰ç¼€
                count = prefix_counts[prefix]
                report += f"   {i+1}. {prefix} ({count} ä¸ªå¼ é‡)\n"
            if len(unique_prefixes) > 10:
                report += f"   ... è¿˜æœ‰ {len(unique_prefixes) - 10} ä¸ªå‰ç¼€\n\n"
            else:
                report += "\n"
        
        report += f"{'â”€' * hang}\n"  # åˆ†éš”çº¿
        # æ˜¾ç¤ºå‰å‡ ä¸ªå”¯ä¸€çš„ç¬¬äºŒå‰ç¼€ï¼ˆç¬¬ä¸€æ¬¡å‡ºç°çš„ç¬¬äºŒå‰ç¼€ï¼‰
        if unique_second_prefixes:
            report += f"ğŸ·ï¸ ç¬¬äºŒå‰ç¼€ç»Ÿè®¡ (æŒ‰é¦–æ¬¡å‡ºç°é¡ºåº):\n"
            for i, prefix in enumerate(unique_second_prefixes[:10]):  # æ˜¾ç¤ºå‰10ä¸ªä¸åŒçš„ç¬¬äºŒå‰ç¼€
                count = second_prefix_counts[prefix]
                report += f"   {i+1}. {prefix} ({count} ä¸ªå¼ é‡)\n"
            if len(unique_second_prefixes) > 10:
                report += f"   ... è¿˜æœ‰ {len(unique_second_prefixes) - 10} ä¸ªç¬¬äºŒå‰ç¼€\n\n"
            else:
                report += "\n"
        
        report += f"{'â”€' * hang}\n"  # åˆ†éš”çº¿
        # æ˜¾ç¤ºå‰å‡ ä¸ªå”¯ä¸€çš„ç¬¬ä¸‰å‰ç¼€ï¼ˆç¬¬ä¸€æ¬¡å‡ºç°çš„ç¬¬ä¸‰å‰ç¼€ï¼‰
        if unique_third_prefixes:
            report += f"ğŸ·ï¸ ç¬¬ä¸‰å‰ç¼€ç»Ÿè®¡ (æŒ‰é¦–æ¬¡å‡ºç°é¡ºåº):\n"
            for i, prefix in enumerate(unique_third_prefixes[:10]):  # æ˜¾ç¤ºå‰10ä¸ªä¸åŒçš„ç¬¬ä¸‰å‰ç¼€
                count = third_prefix_counts[prefix]
                report += f"   {i+1}. {prefix} ({count} ä¸ªå¼ é‡)\n"
            if len(unique_third_prefixes) > 10:
                report += f"   ... è¿˜æœ‰ {len(unique_third_prefixes) - 10} ä¸ªç¬¬ä¸‰å‰ç¼€\n\n"
            else:
                report += "\n"
        

        
        report += f"{'â”€' * hang}\n"  # åˆ†éš”çº¿
        report += f"ğŸ“Š æ€»å‚æ•°é‡: {total_params:,} ({format_param_count_practical(total_params)})\n"
        report += f"ğŸ“ˆ å¼ é‡æ•°é‡: {len(reader.tensors)} (å”¯ä¸€åç§°: {unique_name_count}, é‡å¤åç§°: {len(duplicate_names)})\n"
        report += f"ğŸ·ï¸ å‰ç¼€ç»Ÿè®¡: {unique_prefix_count} ä¸ªä¸åŒç¬¬ä¸€å‰ç¼€, {unique_second_prefix_count} ä¸ªä¸åŒç¬¬äºŒå‰ç¼€, {unique_third_prefix_count} ä¸ªä¸åŒç¬¬ä¸‰å‰ç¼€\n"
        report += f" æ˜¾å­˜ä¼°ç®—: {memory_gb:.1f} GB (åŸºäºå…¬å¼: M = (P Ã— Q) / 8 Ã— 1.2)\n"
        report += f"   - P = {total_params / 1_000_000_000:.1f}B\n"
        report += f"   - Q = {Q_value} (ä¸»è¦æ ¼å¼: {main_dtype})\n\n"
        

        # æ˜¾ç¤ºå„ç±»å‹å‚æ•°
        sorted_dtypes = sorted(dtype_param_count.items(), key=lambda x: x[1], reverse=True)
        for dtype, param_count in sorted_dtypes:    
            percentage = (param_count / total_params) * 100
            formatted_count = format_param_count_practical(param_count)
            report += f"ğŸ”¹ {dtype}: {param_count:,} å‚æ•° ({formatted_count}, {percentage:.2f}%)\n"


        report += f"\n{'â”€' * hang}\n"  # åˆ†éš”çº¿
        
        # ä¿å­˜åˆ†æç»“æœåˆ° .checkinfo æ–‡ä»¶
        checkinfo_filename = path.rsplit('.', 1)[0] + '.checkinfo'
        try:
            with open(checkinfo_filename, 'w', encoding='utf-8') as f:
                f.write(report)
            print(report)
            print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {checkinfo_filename}")
        except Exception as e:
            print(report)
            print(f"âš ï¸  ä¿å­˜ .checkinfo æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        
        return dtype_param_count, total_params, memory_gb
        
    except Exception as e:
        error_msg = f"âŒ æ— æ³•è¯»å–GGUFæ–‡ä»¶ {path}:\n{str(e)}"
        print(error_msg)
        return {}, 0, 0

def on_drop(event):
    
    text_box.delete(1.0, tk.END)
    sys.stdout = StdoutRedirector(text_box)
    
    filepath = event.data.strip("{}")
    if filepath.endswith(".gguf"):
        print("è¯»å– GGUF æ–‡ä»¶-"+filepath)
        inspect_gguf(filepath)
    elif filepath.endswith(".safetensors"):
        print("è¯»å– Safetensors æ–‡ä»¶-"+filepath)
        dtype_count, total_params, memory_gb = inspect_safetensors(filepath)
        
        if total_params > 0:
            model_size = classify_model_size(total_params)
            print(f"ğŸ·ï¸ æ¨¡å‹è§„æ¨¡åˆ†ç±»: {model_size}")
    else:
        pass


    # text_box.insert(tk.END, result)

# åˆ›å»ºçª—å£
root = TkinterDnD.Tk()
root.title("Allç²¾åº¦æ£€æŸ¥å·¥å…·")
#ç½®é¡¶çª—å£
root.attributes("-topmost", True)
root.geometry("480x800")

label = tk.Label(root, text="å°† .gguf æˆ– .safetensors æ¨¡å‹æ–‡ä»¶æ‹–æ‹½åˆ°è¿™é‡Œ", bg="#e0e0e0", relief="ridge", height=5)
label.pack(fill="both", padx=10, pady=10, expand=True)
label.drop_target_register(DND_FILES)
label.dnd_bind('<<Drop>>', on_drop)

# åˆ›å»ºæ–‡æœ¬æ¡†å’Œæ»šåŠ¨æ¡å®¹å™¨
text_frame = tk.Frame(root)
text_frame.pack(fill="both", padx=10, pady=10, expand=True)

# åˆ›å»ºæ–‡æœ¬æ¡†
text_box = tk.Text(text_frame, wrap=tk.WORD)

# åˆ›å»ºå‚ç›´æ»šåŠ¨æ¡
v_scrollbar = tk.Scrollbar(text_frame, orient="vertical", command=text_box.yview)

# é…ç½®æ–‡æœ¬æ¡†çš„æ»šåŠ¨æ¡
text_box.config(yscrollcommand=v_scrollbar.set)

# å¸ƒå±€
text_box.grid(row=0, column=0, sticky="nsew")
v_scrollbar.grid(row=0, column=1, sticky="ns")

# é…ç½®ç½‘æ ¼æƒé‡
text_frame.grid_rowconfigure(0, weight=1)
text_frame.grid_columnconfigure(0, weight=1)

root.mainloop()
